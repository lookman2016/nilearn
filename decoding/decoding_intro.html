
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml" lang="en">
  <head>
    <meta charset="utf-8" />
    <title>Nilearn: Machine learning for NeuroImaging in Python &#8212; Machine learning for NeuroImaging</title>
    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/gallery.css" />
    <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/language_data.js"></script>
    <script type="text/javascript" src="../_static/copybutton.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="2.2. Choosing the right predictive model" href="estimator_choice.html" />
    <link rel="prev" title="2. Decoding and MVPA: predicting from brain images" href="index.html" />
<meta content="True" name="HandheldFriendly">
<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0">
<meta name="keywords" content="nilearn, neuroimaging, python, neuroscience, machinelearning">


<script type="text/javascript">
function updateTopMenuPosition(height, width) {
    if($(window).scrollTop() > height && $(window).outerWidth() > 1024) {
        //begin to scroll
        $('.related-wrapper').css("z-index", 1000);
        $('.related-wrapper').css("position", "sticky");
        $('.related-wrapper').css("top", 0);
        $('.related-wrapper').css("width", width)
    } else {
        //lock it back into place
        $('.related-wrapper').css("position", "relative");
        $('.related-wrapper').css("top", 0)
    }
}

$(function() {
    var banner_height = $('#logo-banner').outerHeight();
    var banner_width = $('#logo-banner').outerWidth();
    var width = $('.related-wrapper').css("height", $('.related').outerHeight());

    updateTopMenuPosition(banner_height, width);

    $(window).scroll(function(event) {
        updateTopMenuPosition(banner_height, width)
    });

    $(window).resize(function(event) {
        var banner_width = $('#logo-banner').outerWidth();
        var menu_height = $('.related').outerHeight();
        $('.related').css("width", banner_width);
        $('.related-wrapper').css("height", menu_height);
        updateTopMenuPosition(banner_height, width)
    })
});
</script>
<script type="text/javascript">
function updateSideBarPosition(top, offset, sections) {
    var pos = $(window).scrollTop();
    // Lock the table of content to a fixed position once we scroll enough
    var topShift = 2 * offset;
    if(pos > top + topShift + 1) {
        // begin to scroll with sticky menu bar
        var topShift = -topShift + 1;
        if ($(window).outerWidth() < 1024) {
            // compensate top menu that disappears
            topShift -= offset + 1
        }
        $('.sphinxsidebarwrapper').css("position", "fixed");
        $('.sphinxsidebarwrapper').css("top", topShift)
    }
    else {
        //lock it back into place
        $('.sphinxsidebarwrapper').css("position", "relative");
        $('.sphinxsidebarwrapper').css("top",0)
    }

    // Highlight the current section
    i = 0;
    current_section = 0;
    $('a.internal').removeClass('active');
    for(i in sections) {
        if(sections[i] > pos) {
            break
        }
        if($('a.internal[href$="' + i + '"]').is(':visible')){
            current_section = i
        }
    }
    $('a.internal[href$="' + current_section + '"]').addClass('active');
    $('a.internal[href$="' + current_section + '"]').parent().addClass('active')
}

$(function () {
    // Lock the table of content to a fixed position once we scroll enough
    var tocOffset = $('.related-wrapper').outerHeight();
    var marginTop = parseFloat($('.sphinxsidebarwrapper').css('margin-top').replace(/auto/, 0));
    var top = $('.sphinxsidebarwrapper').offset().top - marginTop;
    sections = {};
    url = document.URL.replace(/#.*$/, "");

    // Grab positions of our sections
    $('.headerlink').each(function(){
        sections[this.href.replace(url, '')] = $(this).offset().top - 50
    });

    updateSideBarPosition(top, tocOffset, sections);

    $(window).scroll(function(event) {
        updateSideBarPosition(top, tocOffset, sections)
    });

    $(window).resize(function(event) {
        tocOffset = $('.related-wrapper').outerHeight();
        updateSideBarPosition(top, tocOffset, sections)
    });
});
</script>


<script type="text/javascript">

        var _gaq = _gaq || [];
        _gaq.push(['_setAccount', 'UA-41920728-1']);
        _gaq.push(['_trackPageview']);

        (function() {
            var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
            ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
        })();
    </script>

  </head><body>
<div id="logo-banner">
  <div class="logo">
    <a href="../index.html">
      <img src="../_static/nilearn-logo.png" alt="Nilearn logo"  border="0" />
    </a>
  </div>
  <!-- A tag cloud to make it easy for people to find what they are
                         looking for -->
 <div class="tags">
  <ul>
    <li>
      <big><a href="../auto_examples/decoding/plot_haxby_anova_svm.html">SVM</a></big>
    </li>
    <li>
      <small><a href="../connectivity/parcellating.html">Ward
          clustering</a></small>
    </li>
    <li>
      <a href="searchlight.html">Searchlight</a>
    </li>
    <li>
      <big><a href="../connectivity/resting_state_networks.html">ICA</a></big>
    </li>
    <li>
      <a href="../manipulating_images/data_preparation.html">Nifti IO</a>
    </li>
    <li>
      <a href="../modules/reference.html#module-nilearn.datasets">Datasets</a>
    </li>
  </ul>
 </div>

  <div class="banner">
    <h1>Nilearn:</h1>
    <h2>Machine learning for Neuro-Imaging in Python</h2>
  </div>
  <div class="search_form">
    <div class="gcse-search" id="cse" style="width: 100%;"></div>
    <script>
      (function() {
        var cx = '017289614950330089114:elrt9qoutrq';
        var gcse = document.createElement('script');
        gcse.type = 'text/javascript';
        gcse.async = true;
        gcse.src = 'https://cse.google.com/cse.js?cx=' + cx;
        var s = document.getElementsByTagName('script')[0];
        s.parentNode.insertBefore(gcse, s);
      })();
    </script>
  </div>
</div>



<div class=related-wrapper>
    
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="../py-modindex.html" title="Python Module Index"
             >modules</a></li>
        <li class="right" >
          <a href="estimator_choice.html" title="2.2. Choosing the right predictive model"
             accesskey="N">next</a> |</li>
        <li class="right" >
          <a href="index.html" title="2. Decoding and MVPA: predicting from brain images"
             accesskey="P">previous</a> |</li>
<li><a href="../index.html">Nilearn Home</a> |&nbsp;</li>
<li><a href="../user_guide.html">User Guide</a> |&nbsp;</li>
<li><a href="../auto_examples/index.html">Examples</a> |&nbsp;</li>
<li><a href="../modules/reference.html">Reference</a> |&nbsp;</li>
<li id="navbar-about"><a href="../authors.html">About</a>|&nbsp;</li>
<li id="navbar-ecosystem"><a href="http://www.nipy.org/">Nipy ecosystem</a></li>

          <li class="nav-item nav-item-1"><a href="../user_guide.html" >User guide: table of contents</a> &#187;</li>
          <li class="nav-item nav-item-2"><a href="index.html" accesskey="U"><span class="section-number">2. </span>Decoding and MVPA: predicting from brain images</a> &#187;</li> 
      </ul>
    </div>
</div>

      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">


<h4> Giving credit </h4>
  <ul class="simple">
    <li><p>Please consider <a href="../authors.html#citing">citing the
                    papers</a>.</p></li>
  </ul>

  <h3><a href="../index.html">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">2.1. An introduction to decoding</a><ul>
<li><a class="reference internal" href="#loading-and-preparing-the-data">2.1.1. Loading and preparing the data</a><ul>
<li><a class="reference internal" href="#the-haxby-2001-experiment">2.1.1.1. The Haxby 2001 experiment</a></li>
<li><a class="reference internal" href="#loading-the-data-into-nilearn">2.1.1.2. Loading the data into nilearn</a></li>
</ul>
</li>
<li><a class="reference internal" href="#performing-a-simple-decoding-analysis">2.1.2. Performing a simple decoding analysis</a><ul>
<li><a class="reference internal" href="#the-prediction-engine">2.1.2.1. The prediction engine</a><ul>
<li><a class="reference internal" href="#an-estimator-object">2.1.2.1.1. An estimator object</a></li>
<li><a class="reference internal" href="#applying-it-to-data-fit-train-and-predict-test">2.1.2.1.2. Applying it to data: fit (train) and predict (test)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#measuring-prediction-performance">2.1.2.2. Measuring prediction performance</a><ul>
<li><a class="reference internal" href="#cross-validation">2.1.2.2.1. Cross-validation</a></li>
<li><a class="reference internal" href="#choosing-a-good-cross-validation-strategy">2.1.2.2.2. Choosing a good cross-validation strategy</a></li>
<li><a class="reference internal" href="#choice-of-the-prediction-accuracy-measure">2.1.2.2.3. Choice of the prediction accuracy measure</a></li>
<li><a class="reference internal" href="#measuring-the-chance-level">2.1.2.2.4. Measuring the chance level</a></li>
</ul>
</li>
<li><a class="reference internal" href="#visualizing-the-decoder-s-weights">2.1.2.3. Visualizing the decoder’s weights</a></li>
</ul>
</li>
<li><a class="reference internal" href="#decoding-without-a-mask-anova-svm">2.1.3. Decoding without a mask: Anova-SVM</a><ul>
<li><a class="reference internal" href="#dimension-reduction-with-feature-selection">2.1.3.1. Dimension reduction with feature selection</a></li>
<li><a class="reference internal" href="#visualizing-the-results">2.1.3.2. Visualizing the results</a></li>
</ul>
</li>
<li><a class="reference internal" href="#going-further-with-scikit-learn">2.1.4. Going further with scikit-learn</a><ul>
<li><a class="reference internal" href="#changing-the-prediction-engine">2.1.4.1. Changing the prediction engine</a></li>
<li><a class="reference internal" href="#changing-the-feature-selection">2.1.4.2. Changing the feature selection</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  <h4>Previous topic</h4>
  <p class="topless"><a href="index.html"
                        title="previous chapter"><span class="section-number">2. </span>Decoding and MVPA: predicting from brain images</a></p>
  <h4>Next topic</h4>
  <p class="topless"><a href="estimator_choice.html"
                        title="next chapter"><span class="section-number">2.2. </span>Choosing the right predictive model</a></p>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="an-introduction-to-decoding">
<span id="decoding-intro"></span><h1><span class="section-number">2.1. </span>An introduction to decoding<a class="headerlink" href="#an-introduction-to-decoding" title="Permalink to this headline">¶</a></h1>
<p>This section gives an introduction to the main concept of decoding:
predicting from brain images.</p>
<p>The discussion and examples are articulated on the analysis of the Haxby
2001 dataset, showing how to predict from fMRI images the stimuli that
the subject is viewing. However the process is the same in other settings
predicting from other brain imaging modalities, for instance predicting
phenotype or diagnostic status from VBM (Voxel Based Morphometry) maps
(as illustrated in <a class="reference internal" href="../auto_examples/02_decoding/plot_oasis_vbm.html#sphx-glr-auto-examples-02-decoding-plot-oasis-vbm-py"><span class="std std-ref">a more complex example</span></a>), or from FA maps
to capture diffusion mapping.</p>
<div class="contents local topic" id="contents">
<p class="topic-title"><strong>Contents</strong></p>
<ul class="simple">
<li><p><a class="reference internal" href="#loading-and-preparing-the-data" id="id6">Loading and preparing the data</a></p></li>
<li><p><a class="reference internal" href="#performing-a-simple-decoding-analysis" id="id7">Performing a simple decoding analysis</a></p></li>
<li><p><a class="reference internal" href="#decoding-without-a-mask-anova-svm" id="id8">Decoding without a mask: Anova-SVM</a></p></li>
<li><p><a class="reference internal" href="#going-further-with-scikit-learn" id="id9">Going further with scikit-learn</a></p></li>
</ul>
</div>
<div class="section" id="loading-and-preparing-the-data">
<h2><a class="toc-backref" href="#id6"><span class="section-number">2.1.1. </span>Loading and preparing the data</a><a class="headerlink" href="#loading-and-preparing-the-data" title="Permalink to this headline">¶</a></h2>
<div class="section" id="the-haxby-2001-experiment">
<h3><span class="section-number">2.1.1.1. </span>The Haxby 2001 experiment<a class="headerlink" href="#the-haxby-2001-experiment" title="Permalink to this headline">¶</a></h3>
<p>In the Haxby experiment,
subjects were presented visual stimuli from different categories. We are
going to predict which category the subject is seeing from the fMRI
activity recorded in masks of the ventral stream. Significant prediction
shows that the signal in the region contains information on the
corresponding category.</p>
<div class="figure align-left" id="id1">
<a class="reference external image-reference" href="../auto_examples/02_decoding/plot_haxby_stimuli.html"><img alt="decoding/../auto_examples/02_decoding/images/sphx_glr_plot_haxby_stimuli_004.png" src="decoding/../auto_examples/02_decoding/images/sphx_glr_plot_haxby_stimuli_004.png" /></a>
<p class="caption"><span class="caption-text">Face stimuli</span><a class="headerlink" href="#id1" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-left" id="id2">
<a class="reference external image-reference" href="../auto_examples/02_decoding/plot_haxby_stimuli.html"><img alt="decoding/../auto_examples/02_decoding/images/sphx_glr_plot_haxby_stimuli_002.png" src="decoding/../auto_examples/02_decoding/images/sphx_glr_plot_haxby_stimuli_002.png" /></a>
<p class="caption"><span class="caption-text">Cat stimuli</span><a class="headerlink" href="#id2" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-left" id="id3">
<a class="reference external image-reference" href="../auto_examples/01_plotting/plot_haxby_masks.html"><img alt="decoding/../auto_examples/01_plotting/images/sphx_glr_plot_haxby_masks_001.png" src="decoding/../auto_examples/01_plotting/images/sphx_glr_plot_haxby_masks_001.png" /></a>
<p class="caption"><span class="caption-text">Masks</span><a class="headerlink" href="#id3" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-left" id="id4">
<a class="reference external image-reference" href="../auto_examples/02_decoding/plot_haxby_full_analysis.html"><img alt="decoding/../auto_examples/02_decoding/images/sphx_glr_plot_haxby_full_analysis_001.png" src="decoding/../auto_examples/02_decoding/images/sphx_glr_plot_haxby_full_analysis_001.png" /></a>
<p class="caption"><span class="caption-text">Decoding scores per mask</span><a class="headerlink" href="#id4" title="Permalink to this image">¶</a></p>
</div>
<hr class="docutils" />
<div class="topic">
<p class="topic-title"><strong>fMRI: using beta maps of a first-level analysis</strong></p>
<p>The Haxby experiment is unusual because the experimental paradigm is
made of many blocks of continuous stimulation. Most cognitive
experiments have a more complex temporal structure with rich sequences
of events.</p>
<p>The standard approach to decoding consists in fitting a first-level
GLM to retrieve one response map (a beta map) per trial. This is
sometimes known as “beta-series regressions” (see Mumford et al,
<em>Deconvolving bold activation in event-related designs for multivoxel
pattern classification analyses</em>, NeuroImage 2012). These maps can
then be input to the decoder as below, predicting the conditions
associated to trial.</p>
<p>For simplicity, we will work on the raw time-series of the data.
However, <strong>it is strongly recomended that you fit a first level to
include an HRF model and isolate the responses from various
confounds</strong>.</p>
</div>
</div>
<div class="section" id="loading-the-data-into-nilearn">
<h3><span class="section-number">2.1.1.2. </span>Loading the data into nilearn<a class="headerlink" href="#loading-the-data-into-nilearn" title="Permalink to this headline">¶</a></h3>
<div class="topic">
<p class="topic-title"><strong>Full code example</strong></p>
<p>The documentation here just gives the big idea. A full code example,
with explanation, can be found on
<a class="reference internal" href="../auto_examples/plot_decoding_tutorial.html#sphx-glr-auto-examples-plot-decoding-tutorial-py"><span class="std std-ref">A introduction tutorial to fMRI decoding</span></a></p>
</div>
<ul class="simple">
<li><p><strong>Starting an environment</strong>: Launch IPython via “ipython –matplotlib”
in a terminal, or use the Jupyter notebook.</p></li>
<li><p><strong>Retrieving the data</strong>: In the tutorial, we load the data using nilearn
data downloading function, <a class="reference internal" href="../modules/generated/nilearn.datasets.fetch_haxby.html#nilearn.datasets.fetch_haxby" title="nilearn.datasets.fetch_haxby"><code class="xref py py-func docutils literal notranslate"><span class="pre">nilearn.datasets.fetch_haxby</span></code></a>.
However, all this function does is to download the data and return
paths to the files downloaded on the disk. To input your own data to
nilearn, you can pass in the path to your own files
(<a class="reference internal" href="../manipulating_images/input_output.html#loading-data"><span class="std std-ref">more on data input</span></a>).</p></li>
<li><p><strong>Loading the behavioral labels</strong>: Behavioral information is often stored
in a text file such as a CSV, and must be load with
<strong>numpy.recfromcsv</strong> or <a class="reference external" href="http://pandas.pydata.org/">pandas</a></p></li>
<li><p><strong>Extracting the fMRI data</strong>: we then use the
<a class="reference internal" href="../modules/generated/nilearn.input_data.NiftiMasker.html#nilearn.input_data.NiftiMasker" title="nilearn.input_data.NiftiMasker"><code class="xref py py-class docutils literal notranslate"><span class="pre">nilearn.input_data.NiftiMasker</span></code></a>: we extract only the voxels on
the mask of the ventral temporal cortex that comes with the data,
applying the <cite>mask_vt</cite> mask to
the 4D fMRI data. The resulting data is then a matrix with a shape that is
(n_timepoints, n_voxels)
(see <a class="reference internal" href="../manipulating_images/manipulating_images.html#mask-4d-2-3d"><span class="std std-ref">Masking data: from 4D Nifti images to 2D data arrays</span></a> for a discussion on using masks).</p></li>
<li><p><strong>Sample mask</strong>: Masking some of the time points may be useful to
restrict to a specific pair of conditions (<em>eg</em> cats versus faces).</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Seemingly minor data preparation can matter a lot on the final score,
for instance standardizing the data.</p>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="../manipulating_images/input_output.html#loading-data"><span class="std std-ref">Inputing data: file names or image objects</span></a></p></li>
<li><p><a class="reference internal" href="../building_blocks/manual_pipeline.html#masking"><span class="std std-ref">Masking the data: from 4D image to 2D array</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="section" id="performing-a-simple-decoding-analysis">
<h2><a class="toc-backref" href="#id7"><span class="section-number">2.1.2. </span>Performing a simple decoding analysis</a><a class="headerlink" href="#performing-a-simple-decoding-analysis" title="Permalink to this headline">¶</a></h2>
<div class="section" id="the-prediction-engine">
<h3><span class="section-number">2.1.2.1. </span>The prediction engine<a class="headerlink" href="#the-prediction-engine" title="Permalink to this headline">¶</a></h3>
<div class="section" id="an-estimator-object">
<h4><span class="section-number">2.1.2.1.1. </span>An estimator object<a class="headerlink" href="#an-estimator-object" title="Permalink to this headline">¶</a></h4>
<p>To perform decoding we need to use an estimator from the <cite>scikit-learn
&lt;http://scikit-learn.org&gt;</cite> machine-learning library. This object can
predict a condition label <strong>y</strong> given a set <strong>X</strong> of imaging data.</p>
<p>A simple and yet performant choice is the <a class="reference external" href="http://scikit-learn.org/stable/modules/svm.html">Support Vector Classifier</a> (or SVC) with a
linear kernel. The corresponding class, <a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="(in scikit-learn v0.22)"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.svm.SVC</span></code></a>, needs
to be imported from the scikit-learn.</p>
<p>Note that the documentation of the object details all parameters. In
IPython, it can be displayed as follows:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>In [10]: svc?
Type:             SVC
Base Class:       &lt;class &#39;sklearn.svm.libsvm.SVC&#39;&gt;
String Form:
SVC(kernel=linear, C=1.0, probability=False, degree=3, coef0=0.0, tol=0.001,
cache_size=200, shrinking=True, gamma=0.0)
Namespace:        Interactive
Docstring:
    C-Support Vector Classification.
    Parameters
    ----------
    C : float, optional (default=1.0)
        penalty parameter C of the error term.
...
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>the <a class="reference external" href="http://scikit-learn.org/stable/modules/svm.html">scikit-learn documentation on SVMs</a></p>
</div>
</div>
<div class="section" id="applying-it-to-data-fit-train-and-predict-test">
<h4><span class="section-number">2.1.2.1.2. </span>Applying it to data: fit (train) and predict (test)<a class="headerlink" href="#applying-it-to-data-fit-train-and-predict-test" title="Permalink to this headline">¶</a></h4>
<p>The prediction objects have two important methods:</p>
<ul class="simple">
<li><p>a <cite>fit</cite> function that “learns” the parameters of the model from the data.
Thus, we need to give some training data to <cite>fit</cite>.</p></li>
<li><p>a <cite>predict</cite> function that “predicts” a target from new data.
Here, we just have to give the new set of images (as the target should be
unknown):</p></li>
</ul>
<div class="admonition warning">
<p class="admonition-title">Warning</p>
<p><strong>Do not predict on data used by the fit: this would yield misleadingly optimistic scores.</strong></p>
</div>
</div>
</div>
<div class="section" id="measuring-prediction-performance">
<h3><span class="section-number">2.1.2.2. </span>Measuring prediction performance<a class="headerlink" href="#measuring-prediction-performance" title="Permalink to this headline">¶</a></h3>
<div class="section" id="cross-validation">
<h4><span class="section-number">2.1.2.2.1. </span>Cross-validation<a class="headerlink" href="#cross-validation" title="Permalink to this headline">¶</a></h4>
<p>We cannot measure a prediction error on the same set of data that we have
used to fit the estimator: it would be much easier than on new data, and
the result would be meaningless. We need to use a technique called
<em>cross-validation</em> to split the data into different sets, called “folds”,
in a <a class="reference external" href="https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation">K-Fold strategy</a>.</p>
<p>There is a specific function,
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html#sklearn.model_selection.cross_val_score" title="(in scikit-learn v0.22)"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.model_selection.cross_val_score</span></code></a> that computes for you
the score for the different folds of cross-validation:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">cross_val_score</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">cv_scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">svc</span><span class="p">,</span> <span class="n">fmri_masked</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>  
</pre></div>
</div>
<p><cite>cv=5</cite> stipulates a 5-fold cross-validation. Note that this function is located
in <cite>sklearn.model_selection.cross_val_score</cite> in the newest version of
scikit-learn.</p>
<p>You can speed up the computation by using n_jobs=-1, which will spread
the computation equally across all processors (but might not work under
Windows):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cv_scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">svc</span><span class="p">,</span> <span class="n">fmri_masked</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> 
</pre></div>
</div>
<p><strong>Prediction accuracy</strong>: We can take a look at the results of the
<cite>cross_val_score</cite> function:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">cv_scores</span><span class="p">)</span>  
<span class="go">[0.72727272727272729, 0.46511627906976744, 0.72093023255813948, 0.58139534883720934, 0.7441860465116279]</span>
</pre></div>
</div>
<p>This is simply the prediction score for each fold, i.e. the fraction of
correct predictions on the left-out data.</p>
</div>
<div class="section" id="choosing-a-good-cross-validation-strategy">
<h4><span class="section-number">2.1.2.2.2. </span>Choosing a good cross-validation strategy<a class="headerlink" href="#choosing-a-good-cross-validation-strategy" title="Permalink to this headline">¶</a></h4>
<p>There are many cross-validation strategies possible, including K-Fold or
leave-one-out. When choosing a strategy, keep in mind that:</p>
<ul class="simple">
<li><p>The test set should be as litte correlated as possible with the train
set</p></li>
<li><p>The test set needs to have enough samples to enable a good measure of
the prediction error (a rule of thumb is to use 10 to 20% of the data).</p></li>
</ul>
<p>In these regards, leave one out is often one of the worst options (see
Varoquaux et al, <em>Assessing and tuning brain decoders: cross-validation,
caveats, and guidelines</em>, Neuroimage 2017).</p>
<p>Here, in the Haxby example, we are going to leave a session out, in order
to have a test set independent from the train set. For this, we are going
to use the session label, present in the behavioral data file, and
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.LeaveOneGroupOut.html#sklearn.model_selection.LeaveOneGroupOut" title="(in scikit-learn v0.22)"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.model_selection.LeaveOneGroupOut</span></code></a>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Full code for the above can be found on
<a class="reference internal" href="../auto_examples/plot_decoding_tutorial.html#sphx-glr-auto-examples-plot-decoding-tutorial-py"><span class="std std-ref">A introduction tutorial to fMRI decoding</span></a></p>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="green topic">
<p class="topic-title"><strong>Exercise</strong></p>
<p>Compute the mean prediction accuracy using <cite>cv_scores</cite>.</p>
</div>
<div class="topic">
<p class="topic-title">Solution</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">classification_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cv_scores</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">classification_accuracy</span>  
<span class="go">0.76851...</span>
</pre></div>
</div>
</div>
<p>For discriminating human faces from cats, we measure a total prediction
accuracy of <em>77%</em> across the different sessions.</p>
</div>
<div class="section" id="choice-of-the-prediction-accuracy-measure">
<h4><span class="section-number">2.1.2.2.3. </span>Choice of the prediction accuracy measure<a class="headerlink" href="#choice-of-the-prediction-accuracy-measure" title="Permalink to this headline">¶</a></h4>
<p>The default metric used for measuring errors is the accuracy score, i.e.
the number of total errors. It is not always a sensible metric,
especially in the case of very imbalanced classes, as in such situations
choosing the dominant class can achieve a low number of errors.</p>
<p>Other metrics, such as the AUC (Area Under the Curve, for the ROC: the
Receiver Operating Characteristic), can be used:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cv_scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">svc</span><span class="p">,</span> <span class="n">fmri_masked</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span>  <span class="n">scoring</span><span class="o">=</span><span class="s1">&#39;roc_auc&#39;</span><span class="p">)</span>  
</pre></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<p>the <a class="reference external" href="http://scikit-learn.org/stable/modules/model_evaluation.html#common-cases-predefined-values">list of scoring options</a></p>
</div>
</div>
<div class="section" id="measuring-the-chance-level">
<h4><span class="section-number">2.1.2.2.4. </span>Measuring the chance level<a class="headerlink" href="#measuring-the-chance-level" title="Permalink to this headline">¶</a></h4>
<p><strong>Dummy estimators</strong>: The simplest way to measure prediction performance
at chance, is to use a <em>“dummy”</em> classifier,
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyClassifier.html#sklearn.dummy.DummyClassifier" title="(in scikit-learn v0.22)"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.dummy.DummyClassifier</span></code></a> (purely random):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.dummy</span> <span class="kn">import</span> <span class="n">DummyClassifier</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">null_cv_scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">DummyClassifier</span><span class="p">(),</span> <span class="n">fmri_masked</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">)</span>  
</pre></div>
</div>
<p><strong>Permutation testing</strong>: A more controlled way, but slower, is to do
permutation testing on the labels, with
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.permutation_test_score.html#sklearn.model_selection.permutation_test_score" title="(in scikit-learn v0.22)"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.model_selection.permutation_test_score</span></code></a>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">permutation_test_score</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">null_cv_scores</span> <span class="o">=</span> <span class="n">permutation_test_score</span><span class="p">(</span><span class="n">svc</span><span class="p">,</span> <span class="n">fmri_masked</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">)</span>  
</pre></div>
</div>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="topic">
<p class="topic-title"><strong>Putting it all together</strong></p>
<p>The <a class="reference internal" href="../auto_examples/02_decoding/plot_haxby_full_analysis.html#sphx-glr-auto-examples-02-decoding-plot-haxby-full-analysis-py"><span class="std std-ref">ROI-based decoding example</span></a> does a decoding analysis per
mask, giving the f1-score of the prediction for each object.</p>
<p>It uses all the notions presented above, with <code class="docutils literal notranslate"><span class="pre">for</span></code> loop to iterate
over masks and categories and Python dictionaries to store the
scores.</p>
</div>
<div class="figure align-left" id="id5">
<a class="reference external image-reference" href="../auto_examples/01_plotting/plot_haxby_masks.html"><img alt="decoding/../auto_examples/01_plotting/images/sphx_glr_plot_haxby_masks_001.png" src="decoding/../auto_examples/01_plotting/images/sphx_glr_plot_haxby_masks_001.png" /></a>
<p class="caption"><span class="caption-text">Masks</span><a class="headerlink" href="#id5" title="Permalink to this image">¶</a></p>
</div>
<div class="figure align-left">
<a class="reference external image-reference" href="../auto_examples/02_decoding/plot_haxby_full_analysis.html"><img alt="decoding/../auto_examples/02_decoding/images/sphx_glr_plot_haxby_full_analysis_001.png" src="decoding/../auto_examples/02_decoding/images/sphx_glr_plot_haxby_full_analysis_001.png" /></a>
</div>
</div>
</div>
<div class="section" id="visualizing-the-decoder-s-weights">
<h3><span class="section-number">2.1.2.3. </span>Visualizing the decoder’s weights<a class="headerlink" href="#visualizing-the-decoder-s-weights" title="Permalink to this headline">¶</a></h3>
<p>We can visualize the weights of the decoder:</p>
<ul class="simple">
<li><p>we first inverse the masking operation, to retrieve a 3D brain volume
of the SVC’s weights.</p></li>
<li><p>we then create a figure and plot as a background the first EPI image</p></li>
<li><p>finally we plot the SVC’s weights after masking the zero values</p></li>
</ul>
<div class="figure align-default">
<a class="reference external image-reference" href="../auto_examples/plot_decoding_tutorial.html"><img alt="../_images/sphx_glr_plot_decoding_tutorial_0022.png" src="../_images/sphx_glr_plot_decoding_tutorial_0022.png" style="width: 416.0px; height: 312.0px;" /></a>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Full code for the above can be found on
<a class="reference internal" href="../auto_examples/plot_decoding_tutorial.html#sphx-glr-auto-examples-plot-decoding-tutorial-py"><span class="std std-ref">A introduction tutorial to fMRI decoding</span></a></p>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="../plotting/index.html#plotting"><span class="std std-ref">Plotting brain images</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="section" id="decoding-without-a-mask-anova-svm">
<h2><a class="toc-backref" href="#id8"><span class="section-number">2.1.3. </span>Decoding without a mask: Anova-SVM</a><a class="headerlink" href="#decoding-without-a-mask-anova-svm" title="Permalink to this headline">¶</a></h2>
<div class="section" id="dimension-reduction-with-feature-selection">
<h3><span class="section-number">2.1.3.1. </span>Dimension reduction with feature selection<a class="headerlink" href="#dimension-reduction-with-feature-selection" title="Permalink to this headline">¶</a></h3>
<p>If we do not start from a mask of the relevant regions, there is a very
large number of voxels and not all are useful for
face vs cat prediction. We thus add a <a class="reference external" href="http://scikit-learn.org/stable/modules/feature_selection.html">feature selection</a>
procedure. The idea is to select the <cite>k</cite> voxels most correlated to the
task.</p>
<p>For this, we need to import the <a class="reference external" href="https://scikit-learn.org/stable/modules/classes.html#module-sklearn.feature_selection" title="(in scikit-learn v0.22)"><code class="xref py py-mod docutils literal notranslate"><span class="pre">sklearn.feature_selection</span></code></a> module and use
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.f_classif.html#sklearn.feature_selection.f_classif" title="(in scikit-learn v0.22)"><code class="xref py py-func docutils literal notranslate"><span class="pre">sklearn.feature_selection.f_classif</span></code></a>, a simple F-score
based feature selection (a.k.a.
<a class="reference external" href="https://en.wikipedia.org/wiki/Analysis_of_variance#The_F-test">Anova</a>),
that we will put before the SVC in a <cite>pipeline</cite>
(<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html#sklearn.pipeline.Pipeline" title="(in scikit-learn v0.22)"><code class="xref py py-class docutils literal notranslate"><span class="pre">sklearn.pipeline.Pipeline</span></code></a>):</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># ------------------</span>
<span class="c1"># Define the prediction function to be used.</span>
<span class="c1"># Here we use a Support Vector Classification, with a linear kernel</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="n">svc</span> <span class="o">=</span> <span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">)</span>

<span class="c1"># Define the dimension reduction to be used.</span>
<span class="c1"># Here we use a classical univariate feature selection based on F-test,</span>
<span class="c1"># namely Anova. When doing full-brain analysis, it is better to use</span>
<span class="c1"># SelectPercentile, keeping 5% of voxels</span>
<span class="c1"># (because it is independent of the resolution of the data).</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectPercentile</span><span class="p">,</span> <span class="n">f_classif</span>
<span class="n">feature_selection</span> <span class="o">=</span> <span class="n">SelectPercentile</span><span class="p">(</span><span class="n">f_classif</span><span class="p">,</span> <span class="n">percentile</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="c1"># We have our classifier (SVC), our feature selection (SelectPercentile),and now,</span>
<span class="c1"># we can plug them together in a *pipeline* that performs the two operations</span>
<span class="c1"># successively:</span>
<span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="n">anova_svc</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s1">&#39;anova&#39;</span><span class="p">,</span> <span class="n">feature_selection</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;svc&#39;</span><span class="p">,</span> <span class="n">svc</span><span class="p">)])</span>

<span class="c1">#############################################################################</span>
<span class="c1"># Fit the decoder and predict</span>
<span class="c1"># ----------------------------</span>
<span class="n">anova_svc</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">conditions</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">anova_svc</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1">#############################################################################</span>
<span class="c1"># Obtain prediction scores via cross validation</span>
<span class="c1"># -----------------------------------------------</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">LeaveOneGroupOut</span><span class="p">,</span> <span class="n">cross_val_score</span>

<span class="c1"># Define the cross-validation scheme used for validation.</span>
<span class="c1"># Here we use a LeaveOneGroupOut cross-validation on the session group</span>
<span class="c1"># which corresponds to a leave-one-session-out</span>
<span class="n">cv</span> <span class="o">=</span> <span class="n">LeaveOneGroupOut</span><span class="p">()</span>

<span class="c1"># Compute the prediction accuracy for the different folds (i.e. session)</span>
<span class="n">cv_scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">anova_svc</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">conditions</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">groups</span><span class="o">=</span><span class="n">session</span><span class="p">)</span>

<span class="c1"># Return the corresponding mean prediction accuracy</span>
<span class="n">classification_accuracy</span> <span class="o">=</span> <span class="n">cv_scores</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>

<span class="c1"># Print the results</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Classification accuracy: </span><span class="si">%.4f</span><span class="s2"> / Chance level: </span><span class="si">%f</span><span class="s2">&quot;</span> <span class="o">%</span>
      <span class="p">(</span><span class="n">classification_accuracy</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">conditions</span><span class="o">.</span><span class="n">unique</span><span class="p">())))</span>
<span class="c1"># Classification accuracy:  0.70370 / Chance level: 0.5000</span>


<span class="c1">#############################################################################</span>
</pre></div>
</div>
<p>We can use our <code class="docutils literal notranslate"><span class="pre">anova_svc</span></code> object exactly as we were using our <code class="docutils literal notranslate"><span class="pre">svc</span></code>
object previously.</p>
</div>
<div class="section" id="visualizing-the-results">
<h3><span class="section-number">2.1.3.2. </span>Visualizing the results<a class="headerlink" href="#visualizing-the-results" title="Permalink to this headline">¶</a></h3>
<p>To visualize the results, we need to:</p>
<ul class="simple">
<li><p>first get the support vectors of the SVC and inverse the feature
selection mechanism</p></li>
<li><p>then, as before, inverse the masking process to retrieve the weights
and plot them.</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="c1"># ----------------------</span>
<span class="c1"># Look at the SVC&#39;s discriminating weights</span>
<span class="n">coef</span> <span class="o">=</span> <span class="n">svc</span><span class="o">.</span><span class="n">coef_</span>
<span class="c1"># reverse feature selection</span>
<span class="n">coef</span> <span class="o">=</span> <span class="n">feature_selection</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">coef</span><span class="p">)</span>
<span class="c1"># reverse masking</span>
<span class="n">weight_img</span> <span class="o">=</span> <span class="n">masker</span><span class="o">.</span><span class="n">inverse_transform</span><span class="p">(</span><span class="n">coef</span><span class="p">)</span>


<span class="c1"># Use the mean image as a background to avoid relying on anatomical data</span>
<span class="kn">from</span> <span class="nn">nilearn</span> <span class="kn">import</span> <span class="n">image</span>
<span class="n">mean_img</span> <span class="o">=</span> <span class="n">image</span><span class="o">.</span><span class="n">mean_img</span><span class="p">(</span><span class="n">func_filename</span><span class="p">)</span>

<span class="c1"># Create the figure</span>
<span class="kn">from</span> <span class="nn">nilearn.plotting</span> <span class="kn">import</span> <span class="n">plot_stat_map</span><span class="p">,</span> <span class="n">show</span>
<span class="n">plot_stat_map</span><span class="p">(</span><span class="n">weight_img</span><span class="p">,</span> <span class="n">mean_img</span><span class="p">,</span> <span class="n">title</span><span class="o">=</span><span class="s1">&#39;SVM weights&#39;</span><span class="p">)</span>

</pre></div>
</div>
<div class="figure align-default">
<a class="reference external image-reference" href="../auto_examples/02_decoding/plot_haxby_anova_svm.html"><img alt="decoding/../auto_examples/02_decoding/images/sphx_glr_plot_haxby_anova_svm_001.png" src="decoding/../auto_examples/02_decoding/images/sphx_glr_plot_haxby_anova_svm_001.png" /></a>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="../plotting/index.html#plotting"><span class="std std-ref">Plotting brain images</span></a></p></li>
</ul>
</div>
<div class="topic">
<p class="topic-title"><strong>Final script</strong></p>
<p>The complete script to do an SVM-Anova analysis can be found as
<a class="reference internal" href="../auto_examples/02_decoding/plot_haxby_anova_svm.html#sphx-glr-auto-examples-02-decoding-plot-haxby-anova-svm-py"><span class="std std-ref">an example</span></a>.</p>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="space_net.html#space-net"><span class="std std-ref">SpaceNet: decoding with spatial structure for better maps</span></a></p></li>
<li><p><a class="reference internal" href="searchlight.html#searchlight"><span class="std std-ref">Searchlight : finding voxels containing information</span></a></p></li>
</ul>
</div>
</div>
</div>
<div class="section" id="going-further-with-scikit-learn">
<h2><a class="toc-backref" href="#id9"><span class="section-number">2.1.4. </span>Going further with scikit-learn</a><a class="headerlink" href="#going-further-with-scikit-learn" title="Permalink to this headline">¶</a></h2>
<p>We have seen a very simple analysis with scikit-learn, but it may be
interesting to explore the <a class="reference external" href="http://scikit-learn.org/stable/supervised_learning.html">wide variety of supervised learning
algorithms in the scikit-learn</a>.</p>
<div class="section" id="changing-the-prediction-engine">
<h3><span class="section-number">2.1.4.1. </span>Changing the prediction engine<a class="headerlink" href="#changing-the-prediction-engine" title="Permalink to this headline">¶</a></h3>
<p>We now see how one can easily change the prediction engine, if needed.
We can try Fisher’s <a class="reference external" href="http://scikit-learn.org/stable/auto_examples/decomposition/plot_pca_vs_lda.html">Linear Discriminant Analysis (LDA)</a></p>
<p>Import the module:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.discriminant_analysis</span> <span class="kn">import</span> <span class="n">LinearDiscriminantAnalysis</span>  
</pre></div>
</div>
<p>Construct the new estimator object and use it in a pipeline:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.pipeline</span> <span class="kn">import</span> <span class="n">Pipeline</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lda</span> <span class="o">=</span> <span class="n">LinearDiscriminantAnalysis</span><span class="p">()</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">anova_lda</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s1">&#39;anova&#39;</span><span class="p">,</span> <span class="n">feature_selection</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;LDA&#39;</span><span class="p">,</span> <span class="n">lda</span><span class="p">)])</span>  
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Import Linear Discriminant Analysis method in “sklearn.lda.LDA” if you are using
scikit-learn older than version 0.17.</p>
</div>
<p>and recompute the cross-validation score:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cv_scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">anova_lda</span><span class="p">,</span> <span class="n">fmri_masked</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">classification_accuracy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">cv_scores</span><span class="p">)</span>  
<span class="gp">&gt;&gt;&gt; </span><span class="n">n_conditions</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">target</span><span class="p">))</span>  <span class="c1"># number of target classes</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Classification accuracy: </span><span class="si">%.4f</span><span class="s2"> / Chance Level: </span><span class="si">%.4f</span><span class="s2">&quot;</span> <span class="o">%</span> \
<span class="gp">... </span>   <span class="p">(</span><span class="n">classification_accuracy</span><span class="p">,</span> <span class="mf">1.</span> <span class="o">/</span> <span class="n">n_conditions</span><span class="p">))</span>  
<span class="go">Classification accuracy: 0.7846 / Chance level: 0.5000</span>
</pre></div>
</div>
</div>
<div class="section" id="changing-the-feature-selection">
<h3><span class="section-number">2.1.4.2. </span>Changing the feature selection<a class="headerlink" href="#changing-the-feature-selection" title="Permalink to this headline">¶</a></h3>
<p>Let’s start by defining a linear SVM as a first classifier:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">LinearSVC</span><span class="p">()</span>
</pre></div>
</div>
<p>Let’s say that you want a more sophisticated feature selection, for example a
<a class="reference external" href="http://scikit-learn.org/stable/modules/feature_selection.html#recursive-feature-elimination">Recursive Feature Elimination (RFE)</a></p>
<p>Import the module:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">RFE</span>
</pre></div>
</div>
<p>Construct your new fancy selection:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rfe</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s1">&#39;linear&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mf">1.</span><span class="p">),</span> <span class="mi">50</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mf">0.25</span><span class="p">)</span>
</pre></div>
</div>
<p>and create a new pipeline, composing the two classifiers <cite>rfe</cite> and <cite>clf</cite>:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">rfe_svc</span> <span class="o">=</span> <span class="n">Pipeline</span><span class="p">([(</span><span class="s1">&#39;rfe&#39;</span><span class="p">,</span> <span class="n">rfe</span><span class="p">),</span> <span class="p">(</span><span class="s1">&#39;svc&#39;</span><span class="p">,</span> <span class="n">clf</span><span class="p">)])</span>
</pre></div>
</div>
<p>and recompute the cross-validation score:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">cv_scores</span> <span class="o">=</span> <span class="n">cross_val_score</span><span class="p">(</span><span class="n">rfe_svc</span><span class="p">,</span> <span class="n">fmri_masked</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">n_jobs</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  
</pre></div>
</div>
<p>But, be aware that this can take <em>A WHILE</em>…</p>
<div class="line-block">
<div class="line"><br /></div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p>The <a class="reference external" href="http://scikit-learn.org">scikit-learn documentation</a>
has very detailed explanations on a large variety of estimators and
machine learning techniques. To become better at decoding, you need
to study it.</p></li>
</ul>
</div>
</div>
</div>
</div>


          </div>
        </div>
      </div>
      <div class="clearer"></div>
    </div>

    <div class="footer">
            &copy; The nilearn developers 2010-2015.
          Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 2.3.1.
        <span style="padding-left: 5ex;">
          <a href="../_sources/decoding/decoding_intro.rst.txt"
        	 rel="nofollow">Show this page source</a>
        </span>
    </div>
  </body>
</html>